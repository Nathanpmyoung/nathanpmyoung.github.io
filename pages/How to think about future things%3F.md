- From [[Fin Moorhouse]]
	- "I think it is worth appreciating the number and depth of insights that FHI can claim significant credit for. In no particular order:
	  
	  The concept of existential risk, and arguments for treating x-risk reduction as a global priority (see: The Precipice)
	  Arguments for x-risk from AI, and other philosophical considerations around superintelligent AI (see: Superintelligence)
	  Arguments for the scope and importance of humanity's long-term future (since called longtermism)
	  Information hazards
	  Observer selection effects and ‘anthropic shadow’
	  Bounding natural extinction rates with statistical methods
	  The vulnerable world hypothesis
	  Moral trade
	  Crucial considerations
	  The unilteralist's curse
	  Dissolving the Fermi paradox
	  The reversal test in applied ethics
	  'Comprehensive AI services' as an alternative to unipolar outcomes
	  The concept of existential hope"
	- [Link](https://forum.effectivealtruism.org/posts/uK27pds7J36asqJPt/future-of-humanity-institute-2005-2024-final-report?commentId=tdrsrKf2CuWiB28Bf)
-